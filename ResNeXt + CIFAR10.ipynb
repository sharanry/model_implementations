{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.cuda as cuda\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeXt(nn.Module):    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ResNeXt, self).__init__()\n",
    "        \n",
    "        self.conv1 = self.common_layers(nn.Conv2d(in_channels=x.size()[2],\n",
    "                      out_channels=64,\n",
    "                      kernel_size=7,\n",
    "                      strides=2,\n",
    "                      padding=\"SAME\"\n",
    "                     ))\n",
    "        \n",
    "        slef.pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def common_layers(self, y):\n",
    "        y = nn.BatchNorm2d(y)\n",
    "        y = nn.LeakyReLU(y)\n",
    "        return y\n",
    "\n",
    "    def num_of_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        \n",
    "        num_features = 1\n",
    "        \n",
    "        for i in size:\n",
    "            num_features *= i\n",
    "        return num_features\n",
    "\n",
    "    def group_conv(self, y, n_channels, strides):\n",
    "\n",
    "        in_channels = y.size()[1]\n",
    "        if cardinality ==1:\n",
    "            return nn.Conv2d(in_channels=in_channels, \n",
    "                             out_channels=n_channels, \n",
    "                             stride=strides, \n",
    "                             kernel_size=(3,3), \n",
    "                             padding=\"SAME\",\n",
    "                             bias=True\n",
    "                            )\n",
    "\n",
    "        # Cardinality should divide n_channel in order for n_channel(s) to be spilt to #cardinality groups\n",
    "        assert not n_channels % cardinality \n",
    "\n",
    "        # No. of layers in each split\n",
    "        d = n_channels // cardinality\n",
    "\n",
    "        # Splitting layers into different groups and convolving. \n",
    "        # Principle idea of ResNeXt\n",
    "        groups = nn.Conv2d(in_channels=d, \n",
    "                           out_channels=n_channels, \n",
    "                           kernel_size=(3,3), \n",
    "                           stride=strides, \n",
    "                           groups=cardinality, \n",
    "                           bias=True, \n",
    "                           padding='SAME'\n",
    "                          )\n",
    "\n",
    "        # Concatinating the groups into a single tensor\n",
    "        y = torch.cat(groups(y))\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def residual_block(self, y, n_channels_in, n_channels_out, strides=(1,1), project_shortcut=False):\n",
    "        \"\"\"\n",
    "        The network is made of multiple residual blocks of similar structure.\n",
    "        There are many skip connections between these blocks to ease training.\n",
    "        \"\"\"\n",
    "\n",
    "        shortcut = y\n",
    "\n",
    "        # WHY???\n",
    "        y = nn.Conv2d(in_channels=y.size()[1],\n",
    "                      out_channels=n_channels_in, \n",
    "                      kernel_size=1, \n",
    "                      stride=1, \n",
    "                      padding='SAME'\n",
    "                     )(y)\n",
    "\n",
    "        y = common_layers(y)\n",
    "\n",
    "        # ResNeXt\n",
    "        y = group_conv(y, n_channels_in, strides=strides)\n",
    "        y = common_layers(y)\n",
    "\n",
    "\n",
    "        y = nn.Conv2d(in_channels=y.size()[1],\n",
    "                          out_channels=n_channels_out, \n",
    "                          kernel_size=1, \n",
    "                          stride=1, \n",
    "                          padding='SAME'\n",
    "                         )(y)\n",
    "        # applying batch norm before adding with the shortcut\n",
    "        y = nn.BatchNorm2d()(y)\n",
    "\n",
    "        # if shortcut is between layers with with differnet sizes\n",
    "        if project_shortcut or strides!=(1,1):\n",
    "\n",
    "            shortcut = nn.Conv2d(in_channels=shortcut.size()[1],\n",
    "                                     out_channels=n_channels_out, \n",
    "                                     kernel_size=1, \n",
    "                                     stride=1, \n",
    "                                     padding='SAME'\n",
    "                                    )(shortcut)\n",
    "\n",
    "            shortcut = nn.BatchNorm2d(shortcut)\n",
    "\n",
    "        y = torch.add(y, shortcut)\n",
    "\n",
    "        y = nn.LeakyReLU(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "    def forward(self, x, cardinality=1):\n",
    "        \"\"\"\n",
    "        If cardinality=1, then ResNet otherwise know as ResNeXt\n",
    "        \"\"\"    \n",
    "    \n",
    "    \n",
    "        \n",
    "        # conv1\n",
    "        x = nn.Conv2d(in_channels=x.size()[2],\n",
    "                      out_channels=64,\n",
    "                      kernel_size=7,\n",
    "                      strides=2,\n",
    "                      padding=\"SAME\"\n",
    "                     )(x)\n",
    "        x = common_layers(x)\n",
    "\n",
    "        # .pool\n",
    "        x = nn.MaxPool2d(kernel_size=2, stride=2)(x)\n",
    "        \n",
    "        # ResNeXt Simplified for the purpose of training\n",
    "        for i in range(3):\n",
    "            project_shortcut = True if i == 0 else False\n",
    "            x = residual_block(x, 128, 256, project_shortcut=project_shortcut)\n",
    "        \n",
    "        features = self.num_of_flat_features(x)\n",
    "        x = x.view(-1, features)\n",
    "        x = F.relu(nn.Linear(features, 120)(x))\n",
    "        x = F.relu(nn.Linear(120, 84)(x))\n",
    "        x = nn.Linear(84, 10)(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNeXt(\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ResNeXt()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tfgpu)",
   "language": "python",
   "name": "tfgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
